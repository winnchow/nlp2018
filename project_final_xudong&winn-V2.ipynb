{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"project_final_xudong&winn-V2.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"}},"cells":[{"metadata":{"id":"-VJW8MHGMyMK","colab_type":"text"},"cell_type":"markdown","source":["# COMP90042 Project 2018: Question Answering – Team: Xudong & Winn\n","## Winn Chow <winnchow> (38315) and Xudong Han <xudongh1 > (881045)\n"]},{"metadata":{"id":"wzyseKpKMyML","colab_type":"text"},"cell_type":"markdown","source":["## Load data from JSON files"]},{"metadata":{"id":"0dVqe0LeMyMM","colab_type":"code","colab":{},"outputId":"d04e9570-bdec-49db-caf3-ac95e5dc9f3c"},"cell_type":"code","source":["import json\n","\n","# Data in JSON format\n","training_json_data = None\n","development_json_data = None\n","documents_json_data = None\n","testing_json_data = None\n","\n","def load_data():\n","    global training_json_data, development_json_data, documents_json_data, testing_json_data\n","    with open('training.json') as json_data:\n","        training_json_data = json.load(json_data)\n","    with open('devel.json') as json_data:\n","        development_json_data = json.load(json_data)\n","    with open('documents.json') as json_data:\n","        documents_json_data = json.load(json_data)\n","    with open('testing.json') as json_data:\n","        testing_json_data = json.load(json_data)\n","\n","load_data()\n","\n","# Testing\n","print(\"<<<Training data>>>\")\n","print(\"Size = \" + str(len(training_json_data)))\n","print(\"1st data: \" + str(training_json_data[0]))\n","print(\"<<<Development data>>>\")\n","print(\"Size = \" + str(len(development_json_data)))\n","print(\"1st data: \" + str(development_json_data[0]))\n","print(\"<<<Document data>>>\")\n","print(\"Size = \" + str(len(documents_json_data)))\n","print(\"1st data: \" + str(documents_json_data[0]))\n","print(\"<<<Testing data>>>\")\n","print(\"Size = \" + str(len(testing_json_data)))\n","print(\"1st data: \" + str(testing_json_data[0]))\n","\n","print(\"Done\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<<<Training data>>>\n","Size = 43379\n","1st data: {'question': 'A kilogram could be definined as having a Planck constant of what value?', 'text': '6966662606895999999♠6.62606896×10−34 j⋅s', 'answer_paragraph': 23, 'docid': 0}\n","<<<Development data>>>\n","Size = 3097\n","1st data: {'question': 'On what date did the companies that became the Computing-Tabulating-Recording Company get consolidated?', 'text': 'june 16 , 1911', 'answer_paragraph': 5, 'docid': 380}\n","<<<Document data>>>\n","Size = 441\n","1st data: {'docid': 0, 'text': ['First recognized in 1900 by Max Planck, it was originally the proportionality constant between the minimal increment of energy, E, of a hypothetical electrically charged oscillator in a cavity that contained black body radiation, and the frequency, f, of its associated electromagnetic wave. In 1905 the value E, the minimal energy increment of a hypothetical oscillator, was theoretically associated by Einstein with a \"quantum\" or minimal element of the energy of the electromagnetic wave itself. The light quantum behaved in some respects as an electrically neutral particle, as opposed to an electromagnetic wave. It was eventually called the photon.', 'Classical statistical mechanics requires the existence of h (but does not define its value). Eventually, following upon Planck\\'s discovery, it was recognized that physical action cannot take on an arbitrary value. Instead, it must be some multiple of a very small quantity, the \"quantum of action\", now called the Planck constant. Classical physics cannot explain this fact. In many cases, such as for monochromatic light or for atoms, this quantum of action also implies that only certain energy levels are allowed, and values in between are forbidden.', 'Equivalently, the smallness of the Planck constant reflects the fact that everyday objects and systems are made of a large number of particles. For example, green light with a wavelength of 555 nanometres (the approximate wavelength to which human eyes are most sensitive) has a frequency of 7014540000000000000♠540 THz (7014540000000000000♠540×1012 Hz). Each photon has an energy E = hf = 6981358000000000000♠3.58×10−19 J. That is a very small amount of energy in terms of everyday experience, but everyday experience is not concerned with individual photons any more than with individual atoms or molecules. An amount of light compatible with everyday experience is the energy of one mole of photons; its energy can be computed by multiplying the photon energy by the Avogadro constant, NA ≈ 7023602200000000000♠6.022×1023 mol−1. The result is that green light of wavelength 555 nm has an energy of 7005216000000000000♠216 kJ/mol, a typical energy of everyday life.', 'In the last years of the nineteenth century, Planck was investigating the problem of black-body radiation first posed by Kirchhoff some forty years earlier. It is well known that hot objects glow, and that hotter objects glow brighter than cooler ones. The electromagnetic field obeys laws of motion similarly to a mass on a spring, and can come to thermal equilibrium with hot atoms. The hot object in equilibrium with light absorbs just as much light as it emits. If the object is black, meaning it absorbs all the light that hits it, then its thermal light emission is maximized.', 'The assumption that black-body radiation is thermal leads to an accurate prediction: the total amount of emitted energy goes up with the temperature according to a definite rule, the Stefan–Boltzmann law (1879–84). But it was also known that the colour of the light given off by a hot object changes with the temperature, so that \"white hot\" is hotter than \"red hot\". Nevertheless, Wilhelm Wien discovered the mathematical relationship between the peaks of the curves at different temperatures, by using the principle of adiabatic invariance. At each different temperature, the curve is moved over by Wien\\'s displacement law (1893). Wien also proposed an approximation for the spectrum of the object, which was correct at high frequencies (short wavelength) but not at low frequencies (long wavelength). It still was not clear why the spectrum of a hot object had the form that it has (see diagram).', \"Prior to Planck's work, it had been assumed that the energy of a body could take on any value whatsoever – that it was a continuous variable. The Rayleigh–Jeans law makes close predictions for a narrow range of values at one limit of temperatures, but the results diverge more and more strongly as temperatures increase. To make Planck's law, which correctly predicts blackbody emissions, it was necessary to multiply the classical expression by a complex factor that involves h in both the numerator and the denominator. The influence of h in this complex factor would not disappear if it were set to zero or to any other value. Making an equation out of Planck's law that would reproduce the Rayleigh–Jeans law could not be done by changing the values of h, of the Boltzmann constant, or of any other constant or variable in the equation. In this case the picture given by classical physics is not duplicated by a range of results in the quantum picture.\", 'The black-body problem was revisited in 1905, when Rayleigh and Jeans (on the one hand) and Einstein (on the other hand) independently proved that classical electromagnetism could never account for the observed spectrum. These proofs are commonly known as the \"ultraviolet catastrophe\", a name coined by Paul Ehrenfest in 1911. They contributed greatly (along with Einstein\\'s work on the photoelectric effect) to convincing physicists that Planck\\'s postulate of quantized energy levels was more than a mere mathematical formalism. The very first Solvay Conference in 1911 was devoted to \"the theory of radiation and quanta\". Max Planck received the 1918 Nobel Prize in Physics \"in recognition of the services he rendered to the advancement of Physics by his discovery of energy quanta\".', 'The photoelectric effect is the emission of electrons (called \"photoelectrons\") from a surface when light is shone on it. It was first observed by Alexandre Edmond Becquerel in 1839, although credit is usually reserved for Heinrich Hertz, who published the first thorough investigation in 1887. Another particularly thorough investigation was published by Philipp Lenard in 1902. Einstein\\'s 1905 paper discussing the effect in terms of light quanta would earn him the Nobel Prize in 1921, when his predictions had been confirmed by the experimental work of Robert Andrews Millikan. The Nobel committee awarded the prize for his work on the photo-electric effect, rather than relativity, both because of a bias against purely theoretical physics not grounded in discovery or experiment, and dissent amongst its members as to the actual proof that relativity was real.', 'Prior to Einstein\\'s paper, electromagnetic radiation such as visible light was considered to behave as a wave: hence the use of the terms \"frequency\" and \"wavelength\" to characterise different types of radiation. The energy transferred by a wave in a given time is called its intensity. The light from a theatre spotlight is more intense than the light from a domestic lightbulb; that is to say that the spotlight gives out more energy per unit time and per unit space(and hence consumes more electricity) than the ordinary bulb, even though the colour of the light might be very similar. Other waves, such as sound or the waves crashing against a seafront, also have their own intensity. However, the energy account of the photoelectric effect didn\\'t seem to agree with the wave description of light.', 'The \"photoelectrons\" emitted as a result of the photoelectric effect have a certain kinetic energy, which can be measured. This kinetic energy (for each photoelectron) is independent of the intensity of the light, but depends linearly on the frequency; and if the frequency is too low (corresponding to a photon energy that is less than the work function of the material), no photoelectrons are emitted at all, unless a plurality of photons, whose energetic sum is greater than the energy of the photoelectrons, acts virtually simultaneously (multiphoton effect)  Assuming the frequency is high enough to cause the photoelectric effect, a rise in intensity of the light source causes more photoelectrons to be emitted with the same kinetic energy, rather than the same number of photoelectrons to be emitted with higher kinetic energy.', \"Niels Bohr introduced the first quantized model of the atom in 1913, in an attempt to overcome a major shortcoming of Rutherford's classical model. In classical electrodynamics, a charge moving in a circle should radiate electromagnetic radiation. If that charge were to be an electron orbiting a nucleus, the radiation would cause it to lose energy and spiral down into the nucleus. Bohr solved this paradox with explicit reference to Planck's work: an electron in a Bohr atom could only have certain defined energies En\", \"Bohr also introduced the quantity , now known as the reduced Planck constant, as the quantum of angular momentum. At first, Bohr thought that this was the angular momentum of each electron in an atom: this proved incorrect and, despite developments by Sommerfeld and others, an accurate description of the electron angular momentum proved beyond the Bohr model. The correct quantization rules for electrons – in which the energy reduces to the Bohr model equation in the case of the hydrogen atom – were given by Heisenberg's matrix mechanics in 1925 and the Schrödinger wave equation in 1926: the reduced Planck constant remains the fundamental quantum of angular momentum. In modern terms, if J is the total angular momentum of a system with rotational invariance, and Jz the angular momentum measured along any given direction, these quantities can only take on the values\", 'where the uncertainty is given as the standard deviation of the measured value from its expected value. There are a number of other such pairs of physically measurable values which obey a similar rule. One example is time vs. energy. The either-or nature of uncertainty forces measurement attempts to choose between trade offs, and given that they are quanta, the trade offs often take the form of either-or (as in Fourier analysis), rather than the compromises and gray areas of time series analysis.', 'The Bohr magneton and the nuclear magneton are units which are used to describe the magnetic properties of the electron and atomic nuclei respectively. The Bohr magneton is the magnetic moment which would be expected for an electron if it behaved as a spinning charge according to classical electrodynamics. It is defined in terms of the reduced Planck constant, the elementary charge and the electron mass, all of which depend on the Planck constant: the final dependence on h1/2 (r2 > 0.995) can be found by expanding the variables.', 'In principle, the Planck constant could be determined by examining the spectrum of a black-body radiator or the kinetic energy of photoelectrons, and this is how its value was first calculated in the early twentieth century. In practice, these are no longer the most accurate methods. The CODATA value quoted here is based on three watt-balance measurements of KJ2RK and one inter-laboratory determination of the molar volume of silicon, but is mostly determined by a 2007 watt-balance measurement made at the U.S. National Institute of Standards and Technology (NIST). Five other measurements by three different methods were initially considered, but not included in the final refinement as they were too imprecise to affect the result.', 'There are both practical and theoretical difficulties in determining h. The practical difficulties can be illustrated by the fact that the two most accurate methods, the watt balance and the X-ray crystal density method, do not appear to agree with one another. The most likely reason is that the measurement uncertainty for one (or both) of the methods has been estimated too low – it is (or they are) not as precise as is currently believed – but for the time being there is no indication which method is at fault.', 'The theoretical difficulties arise from the fact that all of the methods except the X-ray crystal density method rely on the theoretical basis of the Josephson effect and the quantum Hall effect. If these theories are slightly inaccurate – though there is no evidence at present to suggest they are – the methods would not give accurate values for the Planck constant. More importantly, the values of the Planck constant obtained in this way cannot be used as tests of the theories without falling into a circular argument. Fortunately, there are other statistical ways of testing the theories, and the theories have yet to be refuted.', 'A watt balance is an instrument for comparing two powers, one of which is measured in SI watts and the other of which is measured in conventional electrical units. From the definition of the conventional watt W90, this gives a measure of the product KJ2RK in SI units, where RK is the von Klitzing constant which appears in the quantum Hall effect. If the theoretical treatments of the Josephson effect and the quantum Hall effect are valid, and in particular assuming that RK = h/e2, the measurement of KJ2RK is a direct determination of the Planck constant.', 'The gyromagnetic ratio γ is the constant of proportionality between the frequency ν of nuclear magnetic resonance (or electron paramagnetic resonance for electrons) and the applied magnetic field B: ν = γB. It is difficult to measure gyromagnetic ratios precisely because of the difficulties in precisely measuring B, but the value for protons in water at 7002298150000000000♠25 °C is known to better than one part per million. The protons are said to be \"shielded\" from the applied magnetic field by the electrons in the water molecule, the same effect that gives rise to chemical shift in NMR spectroscopy, and this is indicated by a prime on the symbol for the gyromagnetic ratio, γ′p. The gyromagnetic ratio is related to the shielded proton magnetic moment μ′p, the spin number I (I = 1⁄2 for protons) and the reduced Planck constant.', 'A further complication is that the measurement of γ′p involves the measurement of an electric current: this is invariably measured in conventional amperes rather than in SI amperes, so a conversion factor is required. The symbol Γ′p-90 is used for the measured gyromagnetic ratio using conventional electrical units. In addition, there are two methods of measuring the value, a \"low-field\" method and a \"high-field\" method, and the conversion factors are different in the two cases. Only the high-field value Γ′p-90(hi) is of interest in determining the Planck constant.', 'The Faraday constant F is the charge of one mole of electrons, equal to the Avogadro constant NA multiplied by the elementary charge e. It can be determined by careful electrolysis experiments, measuring the amount of silver dissolved from an electrode in a given time and for a given electric current. In practice, it is measured in conventional electrical units, and so given the symbol F90. Substituting the definitions of NA and e, and converting from conventional electrical units to SI units, gives the relation to the Planck constant.', 'The X-ray crystal density method is primarily a method for determining the Avogadro constant NA but as the Avogadro constant is related to the Planck constant it also determines a value for h. The principle behind the method is to determine NA as the ratio between the volume of the unit cell of a crystal, measured by X-ray crystallography, and the molar volume of the substance. Crystals of silicon are used, as they are available in high quality and purity by the technology developed for the semiconductor industry. The unit cell volume is calculated from the spacing between two crystal planes referred to as d220. The molar volume Vm(Si) requires a knowledge of the density of the crystal and the atomic weight of the silicon used. The Planck constant is given by', 'There are a number of proposals to redefine certain of the SI base units in terms of fundamental physical constants. This has already been done for the metre, which is defined in terms of a fixed value of the speed of light. The most urgent unit on the list for redefinition is the kilogram, whose value has been fixed for all science (since 1889) by the mass of a small cylinder of platinum–iridium alloy kept in a vault just outside Paris. While nobody knows if the mass of the International Prototype Kilogram has changed since 1889 – the value 1 kg of its mass expressed in kilograms is by definition unchanged and therein lies one of the problems – it is known that over such a timescale the many similar Pt–Ir alloy cylinders kept in national laboratories around the world, have changed their relative mass by several tens of parts per million, however carefully they are stored, and the more so the more they have been taken out and used as mass standards. A change of several tens of micrograms in one kilogram is equivalent to the current uncertainty in the value of the Planck constant in SI units.', 'The legal process to change the definition of the kilogram is already underway, but it had been decided that no final decision would be made before the next meeting of the General Conference on Weights and Measures in 2011. (For more detailed information, see kilogram definitions.) The Planck constant is a leading contender to form the basis of the new definition, although not the only one. Possible new definitions include \"the mass of a body at rest whose equivalent energy equals the energy of photons whose frequencies sum to 7050135639273999999♠135639274×1042 Hz\", or simply \"the kilogram is defined so that the Planck constant equals 6966662606895999999♠6.62606896×10−34 J⋅s\".']}\n"],"name":"stdout"},{"output_type":"stream","text":["<<<Testing data>>>\n","Size = 3618\n","1st data: {'question': 'Modern browser support standards-based and defacto what?', 'docid': 410, 'id': 0}\n","Done\n"],"name":"stdout"}]},{"metadata":{"id":"wf-ingQ-MyMR","colab_type":"text"},"cell_type":"markdown","source":["## Load spaCy"]},{"metadata":{"id":"qVIbiVxiMyMS","colab_type":"code","colab":{},"outputId":"bbd44c36-8fcf-4c96-fc1f-3c59dbc06926"},"cell_type":"code","source":["import spacy\n","# You need to download the en model: python -m spacy download en\n","nlp = spacy.load('en')\n","\n","print(\"Done\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"metadata":{"id":"Ben-05fcMyMV","colab_type":"text"},"cell_type":"markdown","source":["## Load NLTK"]},{"metadata":{"id":"8Kgx9UW0MyMW","colab_type":"code","colab":{},"outputId":"c71fd4f0-0cc0-4e30-c42f-24d0a95d78b9"},"cell_type":"code","source":["import nltk\n","sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n","#nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","stopwords = set(stopwords.words('english'))\n","lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n","word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n","\n","ans_types_set = [\"NN\",\"NNP\",\"NNS\",\"NNPS\",\"JJ\",\"JJR\",\"JJS\"]\n","noun_set = [\"NN\",\"NNP\",\"NNS\",\"NNPS\"]\n","\n","print(\"Done\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"metadata":{"id":"RBkn1H_dVJPY","colab_type":"text"},"cell_type":"markdown","source":["## Evaluation Functions on Training and Development set"]},{"metadata":{"id":"dGP0XJ5nVJPa","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","Description: Evaluate the prediction of development questions\n","Input: (1) prediction\n","Output: (1) F1 score\n","''' \n","def f1_score_evaluation(prediction_result):\n","    global development_answer_data\n","    if len(prediction_result) != len(development_answer_data):\n","        print(\"Please check the prediction structure\")\n","    else:\n","        result = []\n","        for i in range(len(prediction_result)):\n","            answer = development_answer_data[i][\"text\"].split()\n","            prediction = prediction_result[i][1].split()\n","            tp = len([word for word in answer if word in prediction])\n","            try:\n","                precision = tp/len(prediction)\n","                recall = tp/len(answer)\n","                f1 = 2*precision*recall/(precision+recall)\n","            except:\n","                f1 = 0.0\n","            result.append(f1)\n","        ave_f1 = np.average(np.array(result))\n","        return ave_f1\n","\n","'''\n","Description: check whether the top k results cover the correct paragraph\n","Input: the value the k \n","Output: the probability that the correct prar belongs to the result list\n","''' \n","def evaluate_top_k(k):\n","    global development_answer_data\n","    correct_prediction = 0\n","    for q in development_answer_data:\n","        docid = q[\"docid\"]\n","        question = q[\"question\"] \n","        _, pred = get_top_k_paragraphs(k, docid, question)\n","        if q[\"ans_para_id\"] in pred[0]:\n","            correct_prediction += 1\n","    return correct_prediction/len(development_answer_data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mTGsBWGHMyMZ","colab_type":"text"},"cell_type":"markdown","source":["## Some helper functions"]},{"metadata":{"id":"44qwOpCMMyMZ","colab_type":"code","colab":{},"outputId":"430eb680-8818-44a1-994d-70e05c331cd3"},"cell_type":"code","source":["from sklearn.externals import joblib\n","from nltk.corpus import wordnet\n","import csv\n","import time\n","\n","'''\n","Description: get time\n","Input: \n","Output: time\n","''' \n","def nowtime():\n","    return time.strftime(\"%Y%m%d-%H%M\", time.localtime())\n","\n","'''\n","Description: Remove some unidentifiable unicode characters\n","Input: (1) text\n","Output: (1) text with some unidentifiable unicode characters removed\n","''' \n","def replace(text):\n","    unicode_letters = [\"♠\"]\n","    for l in unicode_letters:\n","        text = text.replace(l, \"\")\n","    return text\n","\n","'''\n","Description: Save object to files\n","Input: (1) object\n","       (2) filename\n","Output: \n","'''\n","def save_object_to_file(obj, filename):\n","    # It will overwrite\n","    with open(filename, 'wb') as output:  \n","        joblib.dump(obj, output)\n","\n","'''\n","Description: Load object from file\n","Input: (1) filename\n","Output: (1) object\n","'''\n","def load_object_from_file(filename):\n","    with open(filename, 'rb') as input:  \n","        obj = joblib.load(input)\n","    return obj\n","\n","'''\n","Description: Normalize text\n","Input: (1) text\n","Output: (1) a list of lowercase lemma with stop words and punctuations removed\n","        (2) spaCy doc of the text\n","'''\n","def normalize(text):\n","    global nlp\n","    \n","    doc = nlp(text)\n","    normalized = []\n","    # Tokenize and normalize\n","    for t in doc:\n","        if not t.is_stop and not t.is_punct:\n","            normalized.append(t.lemma_.lower())\n","    return normalized, doc\n","\n","'''\n","Description: Check if the answer and prediction are the same\n","Input: (1) answer\n","       (2) prediction\n","Output: (1) True - if they are considered to be the same, otherwise False \n","'''\n","def check_answer(answer, pred):\n","    print(\"Correct: \" + answer)\n","    print(\"Predicted: \" + pred)\n","    answer = answer.lower()\n","    pred = pred.lower()\n","    if answer == pred or answer in pred or pred in answer or answer.replace(\" ,\", \",\") == pred:\n","        return True\n","    else:\n","        return False\n","    \n","'''\n","Description: Get lemma\n","Input: (1) word\n","Output: (1) the lemma of the input\n","'''\n","def lemmatize(word):\n","    lemma = lemmatizer.lemmatize(word,'v')\n","    if lemma == word:\n","        lemma = lemmatizer.lemmatize(word,'n')\n","    return lemma\n","\n","'''\n","Description: Get the longest common str\n","Input: (1) str1\n","       (2) str2\n","Output: (1) the longest commcon str\n","        (2) the length of the common str \n","'''\n","def getNumofCommonSubstr(str1, str2):  \n","  \n","    lstr1 = len(str1)  \n","    lstr2 = len(str2)  \n","    record = [[0 for i in range(lstr2+1)] for j in range(lstr1+1)]  # 多一位  \n","    maxNum = 0          # 最长匹配长度  \n","    p = 0               # 匹配的起始位  \n","    for i in range(lstr1):  \n","        for j in range(lstr2):  \n","            if str1[i] == str2[j]:  \n","                # 相同则累加  \n","                record[i+1][j+1] = record[i][j] + 1  \n","                if record[i+1][j+1] > maxNum:  \n","                    # 获取最大匹配长度  \n","                    maxNum = record[i+1][j+1]  \n","                    # 记录最大匹配长度的终止位置  \n","                    p = i + 1  \n","    return str1[p-maxNum:p], maxNum\n","\n","'''\n","Description: Get the number of how many times that st1's words occurence\n","                in str2\n","Input: (1) str1\n","       (2) str2\n","Output: (1) the longest commcon str\n","        (2) the length of the common str \n","'''\n","def getNumofApperance(str1,str2):\n","    ccount_num = 0\n","    str1_token = [i for i in word_tokenizer.tokenize(str1.lower()) if not i in stopwords]\n","    str2_token = [i for i in word_tokenizer.tokenize(str2.lower()) if not i in stopwords]\n","    for i in str1_token:\n","        if i in str2_token:\n","            ccount_num += 1\n","    return ccount_num\n","\n","\"\"\"\n","Description: Save prediction result to files\n","Input: (1) result\n","       (2) filename\n","Output: \n","\"\"\"\n","def save_prediction_to_csv(result,filename):\n","    headers = ['Id','answer']\n","\n","    with open(filename + str(nowtime()) + \".csv\", 'w', encoding = 'utf8') as f:\n","        f_csv = csv.writer(f)\n","        f_csv.writerow(headers)\n","        f_csv.writerows(result)\n","\n","'''\n","Description: Check hypernym relationship\n","Input: (1) hypernym\n","       (2) hyponym\n","Output: (1) True - if they are hypernym-hyponym\n","'''\n","def check_hypernym(hypernym, hyponym):\n","    hyponym_syn = wordnet.synsets(hyponym)\n","    hypernym_syn = wordnet.synsets(hypernym)\n","    \n","    if len(hyponym_syn) == 0 or len(hypernym_syn) == 0:\n","        return False\n","    \n","    for y in hypernym_syn:\n","        for x in hyponym_syn:\n","            while len(x.hypernyms()) != 0:\n","                if x.hypernyms()[0] == y:\n","                    return True\n","                else:\n","                    x = x.hypernyms()[0]\n","    return False\n","\n","'''\n","Description: Get verb and subject from question\n","Input: (1) question\n","Output: (1) verb\n","        (2) subject\n","        (3) the children node of verb\n","'''\n","def get_verb_subj(question):\n","    for token in question:\n","        if token.dep_ in ['nsubj', 'nsubjpass']:  # and token.head == verb\n","            subj = token\n","            verb = subj.head\n","            if verb.pos_ != 'VERB':\n","                continue\n","    return verb, subj, verb.children\n","\n","'''\n","Description: Get answer\n","Input: (1) nlp(sent)\n","       (2) verb\n","       (3) the children node of verb\n","Output: (1) answer\n","'''\n","def process_passage(doc, verb, v_children):\n","    ans = \"\"\n","    for rootmatch in doc:\n","        if rootmatch.lemma_ == verb.lemma_:\n","            #print(list(rootmatch.children))\n","            for i in rootmatch.children:\n","                if i.tag_ in ans_types_set and (not i.text in v_children):\n","                    #print(i.text, i.tag_)\n","                    #print(list(i.subtree))\n","                    for part in i.subtree:\n","                        ans = ans + part.text + \" \"     \n","    return ans\n","\n","'''\n","Description: Get answer\n","Input: (1) nlp(sent)\n","       (2) verb\n","       (3) the children node of verb\n","Output: (1) answer\n","'''\n","def get_answer(question, passage):\n","    verb, subj, v_children = get_verb_subj(nlp(question))\n","    possible_list = [i.text for i in v_children]\n","    try:\n","        answer = process_passage(nlp(passage), verb, possible_list)\n","        return answer.lower()[:-1]\n","    except:\n","        return \"not found\"\n","\n","print(\"Done\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"metadata":{"id":"w7yNRSU3MyMd","colab_type":"text"},"cell_type":"markdown","source":["## Load data into usable format"]},{"metadata":{"id":"Vtn5AMiKMyMd","colab_type":"code","colab":{},"outputId":"06abc88b-4c2f-4b74-a1c2-498d4ff94ede"},"cell_type":"code","source":["# traning data in a list of dictionary format\n","training_answer_data = []\n","\n","for t in training_json_data:\n","    q = {}\n","    q[\"question\"] = t[\"question\"]\n","    q[\"ans_para_id\"] = t[\"answer_paragraph\"]\n","    q[\"ans_para_content\"] = documents_json_data[t[\"docid\"]][\"text\"][t[\"answer_paragraph\"]]\n","    q[\"docid\"] = t[\"docid\"]\n","    q[\"text\"] = t[\"text\"]\n","    q[\"labeled_ans_type\"] = \"\" # To be used\n","    q[\"labeled_ans_type_by\"] = \"\" # To be used\n","    q[\"classified_ans_type\"] = \"\" # To be used\n","    training_answer_data.append(q)\n","\n","# development data in a list of dictionary format\n","development_answer_data = []\n","\n","for t in development_json_data:\n","    q = {}\n","    q[\"question\"] = t[\"question\"]\n","    q[\"ans_para_id\"] = t[\"answer_paragraph\"]\n","    q[\"ans_para_content\"] = documents_json_data[t[\"docid\"]][\"text\"][t[\"answer_paragraph\"]]\n","    q[\"docid\"] = t[\"docid\"]\n","    q[\"text\"] = t[\"text\"]\n","    q[\"labeled_ans_type\"] = \"\" # To be used\n","    q[\"labeled_ans_type_by\"] = \"\" # To be used\n","    q[\"classified_ans_type\"] = \"\" # To be used\n","    development_answer_data.append(q)\n","\n","# testing data in a list of dictionary format\n","testing_answer_data = []\n","\n","for t in testing_json_data:\n","    q = {}\n","    q[\"id\"] = t[\"id\"]\n","    q[\"question\"] = t[\"question\"]\n","    q[\"ans_para_id\"] = \"\" # To be used\n","    q[\"ans_para_content\"] = \"\" # To be used\n","    q[\"docid\"] = t[\"docid\"]\n","    q[\"text\"] = \"\" # To be used\n","    q[\"labeled_ans_type\"] = \"\" # To be used\n","    q[\"labeled_ans_type_by\"] = \"\" # To be used\n","    q[\"classified_ans_type\"] = \"\" # To be used\n","    testing_answer_data.append(q)\n","    \n","# document data in a dictionary format\n","documents_answer_data = {}\n","\n","for d in documents_json_data:\n","    documents_answer_data[d[\"docid\"]] = d[\"text\"]\n","\n","print(\"Done\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"metadata":{"id":"bgMLNHgRMyMh","colab_type":"text"},"cell_type":"markdown","source":["## Build answer type detection classifier"]},{"metadata":{"id":"L8xX1hPlMyMi","colab_type":"code","colab":{},"outputId":"866fcee4-796e-4f6a-aa42-07cbefc00484"},"cell_type":"code","source":["import os.path\n","from tqdm import tqdm\n","from sklearn.feature_extraction import DictVectorizer\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","'''\n","Description: Extract features from a question\n","Input: (1) question\n","Output: (2) features as a list\n","Based on https://shirishkadam.com/2017/07/03/nlp-question-classification-using-support-vector-machines-spacyscikit-learnpandas/\n","'''\n","def process_question(question):\n","    global nlp\n","    \n","    features = {}\n","    doc = nlp(question)\n","    for token in doc:\n","        if token.tag_ in [\"WDT\", \"WP\", \"WP$\", \"WRB\"]:\n","            features[token.tag_] = 1\n","            features[token.lemma_] = 1\n","            if len(doc) > token.i + 1:\n","                features[token.lemma_ + \" \" + doc[token.i + 1].lemma_] = 1\n","                features[doc[token.i + 1].tag_] = 1\n","        if token.dep_ == \"ROOT\":\n","            features[\"root \" + token.tag_] = 1\n","            \n","            if token.text in [\"is\", \"was\", \"are\", \"were\"]:\n","                found_what = False\n","                found_label = False\n","                for c in token.children:\n","                    if \"what\" == c.text.lower():\n","                        found_what = True\n","                    elif c.pos_ == \"NOUN\":\n","                        found_label = True\n","                        label = c.lemma_\n","                    \n","                    # Done\n","                    if found_what and found_label:\n","                        features[\"what be \" + label] = 1\n","                        break\n","\n","            \n","    #print(features)\n","    \n","    return features\n","\n","# Keep a copy\n","training_question_features = None\n","training_answer_type = None\n","\n","'''\n","Description: Detect answer type from training data\n","Input: (1) load_from_file - whether to load from file\n","Output: (1) a list of question features from the training data\n","        (2) a list of answer types from the training data\n","'''\n","def detect_training_ans_type(load_from_file = True):\n","    global nlp, training_question_features, training_answer_type\n","    \n","    filename = \"training_answer_type_final.pkl\"\n","\n","    if training_question_features is not None and training_answer_type is not None:\n","        return (training_question_features, training_answer_type)\n","    \n","    if load_from_file and os.path.isfile(filename):\n","        training_question_features, training_answer_type = load_object_from_file(filename)\n","        return (training_question_features, training_answer_type)\n","    else:\n","        training_question_features = []\n","        training_answer_type = []\n","    \n","    total = 0\n","    for i in tqdm(range(len(training_answer_data))):\n","        doc = nlp(training_answer_data[i][\"ans_para_content\"])\n","        \n","        best_match = 0\n","        best_label = \"\"\n","        for ent in doc.ents:\n","            #print(ent.text, ent.label_)\n","            match = 0\n","            # Tokenize each entity name and match with the answer\n","            for e in nlp(ent.text):\n","                # Ignore stop words and do lower case comparison\n","                if not e.is_stop and e.text.lower() in training_answer_data[i][\"text\"].lower():\n","                    match += 1\n","            # If better match is found\n","            if match > best_match:\n","                best_match = match\n","                best_label = ent.label_\n","    \n","        if best_match > 0:\n","            total += 1\n","            training_answer_data[i][\"labeled_ans_type_by\"] = \"NER\"\n","            training_answer_data[i][\"labeled_ans_type\"] = best_label\n","            training_answer_data[i][\"question_features\"] = process_question(training_answer_data[i][\"question\"])\n","            training_question_features.append(training_answer_data[i][\"question_features\"])\n","            training_answer_type.append(training_answer_data[i][\"labeled_ans_type\"])\n","        else:\n","            training_answer_data[i][\"labeled_ans_type_by\"] = \"UNKOWN\"\n","            training_answer_data[i][\"labeled_ans_type\"] = \"UNKOWN\"\n","            training_answer_data[i][\"question_features\"] = process_question(training_answer_data[i][\"question\"])\n","            training_question_features.append(training_answer_data[i][\"question_features\"])\n","            training_answer_type.append(training_answer_data[i][\"labeled_ans_type\"])\n","\n","    print(\"Total = \" + str(total))\n","    \n","    save_object_to_file((training_question_features, training_answer_type), filename)\n","    \n","    return (training_question_features, training_answer_type)\n","\n","# Keep a copy\n","ml = None\n","\n","'''\n","Description: Train an answer type detection ML classifer using SVC\n","Input: (1) load_from_file - whether to load from file\n","Output: (1) question vectorizer\n","        (2) machine learning model\n","'''\n","def train_answer_type_detection_classifer(load_from_file = True):\n","    global ml\n","    \n","    filename = \"ml_model_final.pkl\"\n","    \n","    if ml is not None:\n","        return ml\n","    \n","    if load_from_file and os.path.isfile(filename):\n","        ml = load_object_from_file(filename)\n","        return ml\n","        \n","    train_feature_matrix, train_target = detect_training_ans_type()\n","    vectorizer = DictVectorizer()\n","    train_dataset = vectorizer.fit_transform(train_feature_matrix)\n","    \n","    clf = SVC(gamma = 0.3, C = 3)\n","    model = clf.fit(train_dataset, train_target)\n","\n","    save_object_to_file((vectorizer, model), filename)\n","    \n","    ml = (vectorizer, model)\n","    \n","    return (vectorizer, model)\n","\n","'''\n","Description: Predict answer type using ML classifier\n","Input: (1) question\n","Ouput: (1) predicted answer type\n","'''\n","def predict_answer_type_ml(question):\n","    # ML classifer\n","    vectorizer, model = train_answer_type_detection_classifer()\n","    f = process_question(question)\n","    vdata = vectorizer.transform(f)    \n","    pred = model.predict(vdata)\n","    \n","    if pred in [\"PERSON\", \"ORG\"]:\n","        return \"who_type\", None\n","    if pred in [\"DATE\"]:\n","        return \"when_type\", None\n","    if pred in [\"GPE\", \"LOC\"]:\n","        return \"where_type\", None\n","    if pred in [\"PERCENT\"]:\n","        return \"what_percentage_type\", None\n","    if pred in [\"CARDINAL\"]:\n","        return \"how_quantity_type\", None\n","    if pred in [\"UNKOWN\"]:\n","        return \"unkown_type\", None\n","    else:\n","        return \"other_label_type\", pred\n","\n","'''\n","Description: Testing answer type detection classifier\n","Input:\n","Output:\n","'''\n","def testing_answer_type_detection_classifer():\n","    \n","    training_question_features, training_answer_type = detect_training_ans_type()\n","    \n","    vectorizer, model = train_answer_type_detection_classifer()\n","    data = []\n","    for i in tqdm(range(len(training_answer_data))):\n","        q = training_answer_data[i][\"question\"]\n","        f = process_question(q)\n","        data.append(f)\n","    vdata = vectorizer.transform(data)    \n","    pred = model.predict(vdata)\n","    print(\"Accuracy: \" + str(accuracy_score(training_answer_type, pred)))\n","    print(classification_report(training_answer_type, pred))\n","    \n","#testing_answer_type_detection_classifer()\n","\n","print(\"Done\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"metadata":{"id":"MkxtP2muMyMw","colab_type":"text"},"cell_type":"markdown","source":["## Check most common question words - for analysis"]},{"metadata":{"id":"r5iMxViiMyMw","colab_type":"code","colab":{},"outputId":"07667948-e3e0-421a-80b7-09799ff4ae4e"},"cell_type":"code","source":["from collections import defaultdict\n","from collections import Counter\n","\n","'''\n","Description: Check most common question words\n","'''\n","def check_most_common_question_words():\n","    global training_answer_type, nlp\n","    \n","    ans_tokens = defaultdict(lambda: [])\n","    ans_counters = {}\n","\n","    for idx, ans in tqdm(enumerate(training_answer_type)):\n","        doc = nlp(training_answer_data[idx][\"question\"])\n","        \n","        bigram = []\n","        for token in doc:\n","            if len(doc) > token.i + 1:\n","                bigram.append(token.text + \" \" + doc[token.i + 1].text)\n","        ans_tokens[ans] = ans_tokens[ans] + bigram\n","    \n","    for ans, ts in ans_tokens.items():\n","        ans_counters[ans] = Counter(ts)\n","        print(ans)\n","        c = ans_counters[ans].most_common(20)\n","        print(c)\n","\n","print(\"Done\")\n","\n","#check_most_common_question_words()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"metadata":{"id":"SpEcQIdpMyM0","colab_type":"text"},"cell_type":"markdown","source":["## Answer type detection classifer"]},{"metadata":{"id":"jrBiisMuMyM1","colab_type":"code","colab":{},"outputId":"83c6e144-4eb2-4778-a64b-2cc119f4b270"},"cell_type":"code","source":["'''\n","Description: Classify a question into an answer type\n","Input: (1) question\n","Output: (1) classified answer type\n","        (2) info about the type if needed\n","'''\n","def classify_answer_type(question):\n","    qcontent = question.lower()\n","    \n","    # Rule-based\n","    if qcontent.startswith(\"who\") or \"what is the name\" in qcontent or \"name of\" in qcontent or \\\n","        \"what scientist\" in qcontent or \"what composer\" in qcontent or \"what job\" in qcontent or \\\n","        \"what artifact\" in qcontent or \"what charity\" in qcontent or \"what university\" in qcontent or \"which university\" in qcontent or \\\n","        \"what agency\" in qcontent or \"what school\" in qcontent or \"which school\" in qcontent  or \"what council\" in qcontent or \\\n","        \"what organization\" in qcontent or \"what local businessman\" in qcontent or \"what name\" in qcontent or \\\n","        \"what building\" in qcontent or \"what museum\" in qcontent or \"what leader\" in qcontent or \"which person\" in qcontent or \\\n","        \"what government agency\" in qcontent or \"which footballer\" in qcontent or \"which actress\" in qcontent or \"which actor\" in qcontent or \\\n","        \"which athlete\" in qcontent or \"which king\" in qcontent or \"which person\" in qcontent:\n","        return \"who_type\", None\n","    if qcontent.startswith(\"when\") or \"what date\" in qcontent or \"how long\" in qcontent or \"how many years\" in qcontent or \\\n","        \"how many hours\" in qcontent or \"what time\" in qcontent or \"what decade\" in qcontent or \"what century\" in qcontent or \\\n","        \"what day\" in qcontent or \"what month\" in qcontent or \"which time\" in qcontent or \"which period\" in qcontent or \\\n","        qcontent.endswith(\"when?\"):\n","        return \"when_type\", None\n","    if qcontent.startswith(\"where\") or \"located where\" in qcontent or \"what country\" in qcontent or \"what city\" in qcontent or \\\n","        \"what nation\" in qcontent or \"what is the location\" in qcontent or \"what empire\" in qcontent or \"which country\" in qcontent or \\\n","        \"what genre of music\" in qcontent or \"what location\" in qcontent or \"what region\" in qcontent or \"what area\" in qcontent or \\\n","        \"which country\" in qcontent or \"which place\" in qcontent or \"which area\" in qcontent or \"which city\" in qcontent or \\\n","        \"which nation\" in qcontent or qcontent.endswith(\"where?\"):\n","        return \"where_type\", None\n","    if \"what percentage\" in qcontent:\n","        return \"what_percentage_type\", None\n","    if \"what year\" in qcontent or \"which year\" in qcontent:\n","        return \"what_year_type\", None\n","    if \"how much\" in qcontent or \"how many\" in qcontent or \"what value\" in qcontent or \\\n","       \"what amount\" in qcontent or \"what number\" in qcontent or \"what size\" in qcontent or \"what quantity\" in qcontent or \\\n","       \"population of\" in qcontent or \"many square\" in qcontent or \"many acres\" in qcontent or \"many miles\" in qcontent or \\\n","       \"what age\" in qcontent or \"which figure\" in qcontent:\n","        return \"how_quantity_type\", None\n","    if \"how much money\" in qcontent or \"does it cost\" in qcontent or \"value of\" in qcontent:\n","        return \"money_type\", None\n","    if \"what rank\" in qcontent:\n","        return \"order_type\", None\n","    if \"what event\" in qcontent or \"what war\" in qcontent or \"what battle\" in qcontent or \"which war\" in qcontent:\n","        return \"event_type\", None\n","    if \"what language\" in qcontent or \"which language\" in qcontent:\n","        return \"language_type\", None\n","    if \"what religion\" in qcontent or \"what nationality\" in qcontent:\n","        return \"norp_type\", None\n","    if \"what town\" in qcontent:\n","        return \"fac_type\", None\n","    \n","    x, _ = check_what_x(question)\n","    if x in [\"color\", \"shape\", \"alloy\", \"animal\", \"metal\", \"weapon\"]:\n","        return \"what_hypernym_type\", x\n","    \n","    # Try ML classifier    \n","    pred_ans_type, info = predict_answer_type_ml(question)\n","    \n","    # print(\">>>>>>>>>>>>>>> ML Prediction: \" + pred_ans_type)\n","\n","    return pred_ans_type, info\n","\n","'''\n","Description: Check if it is a What X question\n","Input: (1) question\n","Output: (1) X of What X, None if it is not a What X question\n","'''\n","def check_what_x(question):\n","    global nlp\n","    \n","    doc = nlp(question)\n","    for chunk in doc.noun_chunks:\n","        c = chunk.text.split()\n","        if len(c) > 1 and c[0].lower() == \"what\":\n","            # Turn X into lemma\n","            d = nlp(\" \".join(c[1:]))\n","            found_noun = False\n","            x = (\"\", None)\n","            for l in d:\n","                # there is a noun, use it\n","                if l.pos_ == \"NOUN\":\n","                    found_noun = True\n","                    x = (l.lemma_, None)\n","                    \n","                    # get What X of Y\n","                    for token in doc:\n","                        if token.lemma_ == x[0]:\n","                            for c in token.children:\n","                                if c.lemma_ == \"of\":\n","                                    for cc in c.children:\n","                                        x = (x[0].lower(), cc.lemma_)\n","                                        break\n","                                    break\n","                            break\n","                    break\n","            if not found_noun:\n","                x = (c[1].lower(), None)\n","                # get What X of Y\n","                for token in doc:\n","                    if token.lemma_ == x[0]:\n","                        for c in token.children:\n","                            if c.lemma_ == \"of\":\n","                                for cc in c.children:\n","                                    x = (x[0].lower(), cc.lemma_)\n","                                    break\n","                                break\n","                        break\n","                break\n","            return x\n","    return (None, None)\n","                \n","print(\"Done\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"metadata":{"id":"CAL7Xmn-MyM3","colab_type":"text"},"cell_type":"markdown","source":["## Build vectorized documents - for passage retrieval"]},{"metadata":{"id":"eTMVICaWMyM4","colab_type":"code","colab":{},"outputId":"f1b7beb8-c03a-4d66-dcdf-e845ffab67f6"},"cell_type":"code","source":["import numpy as np\n","import time\n","import spacy\n","import os.path\n","from tqdm import tqdm\n","from collections import defaultdict\n","from collections import Counter\n","from sklearn.feature_extraction import DictVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","\n","# Keep a copy\n","vectorized_documents = None\n","\n","'''\n","Description: Build vectorized documents and dump to file, vectorized_documents.pkl\n","Input: load_from_file - whether to load from file\n","Output:\n","'''\n","def build_vectorized_documents(load_from_file = True):\n","    global documents_answer_data, nlp, vectorized_documents\n","\n","    filename = \"vectorized_documents.pkl\"\n","    \n","    if vectorized_documents is not None:\n","        return vectorized_documents\n","    \n","    if load_from_file and os.path.isfile(filename):\n","        return load_object_from_file(filename)\n","        \n","    counts = defaultdict(list)\n","    doc_tfidf = defaultdict(lambda: {})\n","    \n","    # Each document\n","    for docid in tqdm(documents_answer_data.keys()):\n","        \n","        # Each paragraph\n","        for para in documents_answer_data[docid]:\n","            p, doc = normalize(para)\n","            # Count\n","            counts[docid].append(Counter(p))\n","    \n","        # Vectorize\n","        vectorizer = DictVectorizer()\n","        doc_count_feature = vectorizer.fit_transform(counts[docid])\n","\n","        # tf-idf\n","        transformer = TfidfTransformer(smooth_idf=True, norm=\"l1\", sublinear_tf=True)\n","        doc_tfidf[docid][\"tfidf\"] = transformer.fit_transform(doc_count_feature)\n","        doc_tfidf[docid][\"vectorizer\"] = vectorizer\n","        doc_tfidf[docid][\"transformer\"] = transformer\n","        \n","    save_object_to_file(doc_tfidf, filename)\n","    \n","    return doc_tfidf\n","\n","vectorized_documents = build_vectorized_documents()\n","\n","print(\"Done\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"metadata":{"id":"Ja4Iq-OTMyM6","colab_type":"text"},"cell_type":"markdown","source":["## Top-K paragraphs retrieval"]},{"metadata":{"id":"UfyDm2QEMyM7","colab_type":"code","colab":{},"outputId":"8d929def-f237-40d0-da9c-e537635e4ff8"},"cell_type":"code","source":["import numpy as np\n","from collections import Counter\n","\n","'''\n","Description: Returns the top-k matched paragraphs from the document\n","Input: (1) k - top k\n","       (2) docid\n","       (3) question\n","Output: (1) a list of top k best matched paragraphs in descending matching order \n","        (2) a list of corresponding similarity scores\n","'''\n","def get_top_k_paragraphs(k, docid, question):\n","    global documents_answer_data\n","    \n","    nq, doc = normalize(question)\n","    q_count = Counter(nq)\n","    doc_tfidf = build_vectorized_documents()\n","    q_count_feature = doc_tfidf[docid][\"vectorizer\"].transform(q_count)\n","    q_count_tfidf = doc_tfidf[docid][\"transformer\"].transform(q_count_feature)\n","    q_score = (doc_tfidf[docid][\"tfidf\"]*q_count_tfidf.T).T\n","\n","    scores = q_score[0,:].toarray()[0]\n","    best = np.argsort(scores)[::-1]\n","    best_ans = best[0:k]\n","    best_scores = scores[best_ans]\n","    \n","    topk_paragraphs = []\n","    for b in best_ans:\n","        topk_paragraphs.append(documents_answer_data[docid][b])\n","    return topk_paragraphs, best_scores\n","\n","print(\"Done\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"metadata":{"id":"IlEbo1PcMyM9","colab_type":"text"},"cell_type":"markdown","source":["## Solver to solve questions - Answer Processing"]},{"metadata":{"id":"UbFSj8x9MyM9","colab_type":"code","colab":{},"outputId":"3bacfdb3-fa60-41f7-aa3b-6dd8782b99be"},"cell_type":"code","source":["import re\n","\n","'''\n","Description: A solver to solve questions\n","Input: (1) question\n","       (2) a list of matched paragraphs\n","       (3) a list of corresponding similarity scores of the matched paragraphs\n","       (4) answer type\n","       (5) answer type info\n","Output: (1) prediction\n","'''\n","def main_solve(question, para_list, para_score_list, ans_type, ans_type_info):\n","    if ans_type == \"who_type\":\n","        return predict_ner(question, para_list, para_score_list, [\"PERSON\", \"ORG\"])\n","    if ans_type == \"when_type\":\n","        return predict_ner(question, para_list, para_score_list, [\"DATE\", \"TIME\"])\n","    if ans_type == \"where_type\":\n","        return predict_ner(question, para_list, para_score_list, [\"GPE\", \"LOC\"])\n","    if ans_type == \"what_percentage_type\":\n","        return predict_ner(question, para_list, para_score_list, [\"PERCENT\"])\n","    if ans_type == \"what_year_type\":\n","        return predict_ner(question, para_list, para_score_list, [\"DATE\"], year = True)\n","    if ans_type == \"how_quantity_type\":\n","        return predict_ner(question, para_list, para_score_list, [\"CARDINAL\", \"QUANTITY\"])\n","    if ans_type == \"money_type\":\n","        return predict_ner(question, para_list, para_score_list, [\"MONEY\"])\n","    if ans_type == \"order_type\":\n","        return predict_ner(question, para_list, para_score_list, [\"ORDINAL\"])\n","    if ans_type == \"event_type\":\n","        return predict_ner(question, para_list, para_score_list, [\"EVENT\"])\n","    if ans_type == \"language_type\":\n","        return predict_ner(question, para_list, para_score_list, [\"LANGUAGE\"])\n","    if ans_type == \"norp_type\":\n","        return predict_ner(question, para_list, para_score_list, [\"NORP\"])\n","    if ans_type == \"fac_type\":\n","        return predict_ner(question, para_list, para_score_list, [\"FAC\"])\n","    if ans_type == \"other_label_type\":\n","        return predict_ner(question, para_list, para_score_list, [ans_type_info])\n","    if ans_type == \"what_hypernym_type\":\n","        return predict_hypernym(question, para_list, para_score_list, ans_type_info)\n","    \n","    return \"Not Found\"\n","\n","def predict_ner(question, para_list, para_score_list, ner_labels, year = False):\n","    global nlp\n","    \n","    all_potential_ans = []\n","    all_text_sentences = []\n","    all_sentences = []\n","    doc_question = nlp(question)\n","    question_ents = [ent.text for ent in doc_question.ents]\n","    # Find all potential answers\n","    for index, para in enumerate(para_list):\n","        potential_ans = []\n","        doc = nlp(para)\n","        for ent in doc.ents:\n","            if not ent.text in question_ents:\n","                if ent.label_ in ner_labels:\n","                    ans = {}\n","                    ans[\"para_idx\"] = index\n","                    ans[\"start\"] = ent.start\n","                    ans[\"end\"] = ent.end\n","                    ans[\"text\"] = ent.text\n","\n","                    if year:\n","                        if len(ans[\"text\"]) == 4 and ans[\"text\"].isnumeric():\n","                            potential_ans.append(ans)\n","                    else:\n","                        potential_ans.append(ans)\n","            #print(potential_ans)\n","        \n","        potential_ans.sort(key = lambda x: x[\"start\"])\n","        \n","        prev_end = 0\n","        prev_ans = None\n","        for i, ans in enumerate(potential_ans):\n","            if i == 0:\n","                ans[\"doc_start\"] = 0\n","                prev_ans = ans\n","            else:\n","                ans[\"doc_start\"] = round((prev_end + ans[\"start\"]) / 2)\n","                prev_ans[\"doc_end\"] = ans[\"doc_start\"]\n","                prev_ans = ans\n","            prev_end = ans[\"end\"]\n","        if len(potential_ans) > 0:\n","            potential_ans[-1][\"doc_end\"] = len(doc)\n","        \n","        all_potential_ans = all_potential_ans + potential_ans\n","        \n","        # Normalize the sentences\n","        sentences = []\n","        text_sentences = []\n","        for ans in potential_ans:\n","            st = []\n","            for t in doc[ans[\"doc_start\"]:ans[\"doc_end\"]]:\n","                if not t.is_stop and not t.is_punct:\n","                    st.append(t.lemma_.lower())\n","            sentences.append(st)\n","            text_sentences.append(doc[ans[\"doc_start\"]:ans[\"doc_end\"]].text)\n","            \n","        all_sentences = all_sentences + sentences\n","        all_text_sentences = all_text_sentences + text_sentences\n","\n","    #print(all_sentences)\n","    \n","    # No potential answers found\n","    if len(all_potential_ans) == 0:\n","        return \"Not Found\"\n","    \n","    counts = []\n","    for s in all_sentences:\n","        counts.append(Counter(s))\n","    \n","    vectorizer = DictVectorizer()\n","    sent_count_feature = vectorizer.fit_transform(counts)\n","\n","    transformer = TfidfTransformer(smooth_idf=True, norm=\"l1\", sublinear_tf=True)\n","    sent_tfidf = transformer.fit_transform(sent_count_feature)\n","        \n","    nq, doc = normalize(question)\n","    q_count = Counter(nq)\n","    \n","    q_count_feature = vectorizer.transform(q_count)\n","    q_count_tfidf = transformer.transform(q_count_feature)\n","    q_score = (sent_tfidf*q_count_tfidf.T).T\n","\n","    scores = q_score[0,:].toarray()[0]\n","    # print(scores)\n","    \n","    best = np.argsort(scores)[::-1]\n","    best_ans = best[0]\n","    \n","    # If the best_ans also appears in the question, choose the second best\n","    for b in best:\n","        if all_potential_ans[b][\"text\"].lower() not in question.lower():\n","            best_ans = b\n","            break\n","\n","    best_ans_text = all_potential_ans[best_ans][\"text\"]\n","    \n","    return best_ans_text\n","\n","\n","'''\n","Description: A specific solver to solve hypernym type questions\n","Input: (1) question\n","       (2) a list of matched paragraphs\n","       (3) a list of corresponding similarity scores of the matched paragraphs\n","       (4) a token\n","Output: (1) prediction or \"Not Found\"\n","'''\n","def predict_hypernym(question, para_list, para_score_list, token):\n","    global nlp\n","    \n","    # Find the best matched sentence\n","    all_text_sentences = []\n","    for index, para in enumerate(para_list):\n","        sentences = []\n","        doc = nlp(para)\n","        for sent in doc.sents:\n","            sentences.append(sent.text)\n","        all_text_sentences = all_text_sentences + sentences\n","    \n","    # No potential answers found\n","    if len(all_text_sentences) == 0:\n","        return \"Not Found\"\n","        \n","    all_sentences = []\n","    for s in all_text_sentences:\n","        st, doc = normalize(s)\n","        all_sentences.append(st)\n","\n","    counts = []\n","    for s in all_sentences:\n","        counts.append(Counter(s))\n","    \n","    vectorizer = DictVectorizer()\n","    sent_count_feature = vectorizer.fit_transform(counts)\n","\n","    transformer = TfidfTransformer(smooth_idf=True, norm=\"l1\", sublinear_tf=True)\n","    sent_tfidf = transformer.fit_transform(sent_count_feature)\n","        \n","    nq, doc = normalize(question)\n","    q_count = Counter(nq)\n","    \n","    q_count_feature = vectorizer.transform(q_count)\n","    q_count_tfidf = transformer.transform(q_count_feature)\n","    q_score = (sent_tfidf*q_count_tfidf.T).T\n","\n","    scores = q_score[0,:].toarray()[0]\n"," \n","    best = np.argsort(scores)[::-1]\n","\n","    all_potential_ans = []\n","    for b in best:\n","        # best matched sentence\n","        best_sentence = all_text_sentences[b]\n","        #print(str(b) + \": \" + best_sentence)\n","        tokens, doc = normalize(best_sentence)\n","        for t in tokens:\n","            if t not in nq and check_hypernym(token, t):\n","                all_potential_ans.append((b, t))\n","\n","    # No potential answers found\n","    if len(all_potential_ans) == 0:\n","        return \"Not Found\"\n","    \n","    sent, best_ans = all_potential_ans[0]\n","    best_sentence = all_text_sentences[sent]\n","    \n","    for t in re.findall(r\"[\\w-]+\", best_sentence):\n","        if best_ans in t:\n","            return t\n","    \n","    return best_ans\n","\n","'''\n","Description: A solver to solve all the other type questions\n","Input: (1) question\n","       (2) a list of matched paragraphs\n","Output: (1) prediction or \"not found\"\n","'''\n","def predict_others(question, para_list):\n","    \n","    ans_para = \"\"\n","    for i in para_list:\n","        ans_para = ans_para+i+\" \"\n","        \n","    max_num = 0\n","    max_sen_num = 0\n","    ans_sentence = \"\"\n","    for sen in sent_segmenter.tokenize(ans_para):\n","        com_str, _ = getNumofCommonSubstr(sen.lower(),question.lower())\n","        app_num = getNumofApperance(sen, question)\n","        sen_score = len(word_tokenizer.tokenize(com_str)) + app_num\n","        if sen_score > max_num:\n","            max_num = sen_score\n","            ans_sentence = sen\n","    ans = \"\"\n","    try:\n","        ans = get_answer(question, ans_sentence)\n","        if len(ans)>2 and (len(ans.split())<5):\n","            return ans.lower()\n","        else:\n","            doc1 = nlp((ans_sentence))\n","            doc2 = nlp(question)\n","            possible_ans = [i.merge().text for i in [chunk for chunk in doc1.noun_chunks] if not i in [chunk for chunk in doc2.noun_chunks]]\n","\n","            # Look for What X and What X of Y\n","            x, y = check_what_x(question)\n","            if x is not None:\n","                for idx, a in enumerate(possible_ans):\n","                    if x in a or (y is not None and y in a):\n","                        #print(\"!!!!!!!!!!!!!!!!!!!!\")\n","                        #print(possible_ans)\n","                        # Get 2 chunks before and after\n","                        if idx > 1:\n","                            start = idx - 1\n","                        elif idx == 1:\n","                            start = 1\n","                        else:\n","                            start = 0\n","                        if len(possible_ans) > idx + 2:\n","                            end = idx + 2\n","                        elif len(possible_ans) > idx + 1:\n","                            end = idx + 2\n","                        else:\n","                            end = idx + 1\n","                        return \" \".join(possible_ans[start:end])\n","\n","            # Check for \"stand for\" and \"refer to\"\n","            found_stand_refer = False\n","            key_word = []\n","            for token in doc2:\n","                if token.dep_ == \"ROOT\" and token.lemma_ in [\"refer\", \"stand\"]:\n","                    found_stand_refer = True\n","                if token.dep_ == \"nsubj\":\n","                    key_word = [t.text for t in list(token.lefts) if not t.is_stop] + [token.text] + [t.text for t in list(token.rights) if not t.is_stop]\n","            if found_stand_refer:\n","                aw = word_tokenizer.tokenize(ans_sentence)\n","                #print(ans_para)\n","                for idx, a in enumerate(aw):\n","                    for kw in key_word:\n","                        if kw in a:\n","                            #print(\"!!!!!!!!!!!!!!!!!!!!\")\n","                            if idx > 5:\n","                                start = idx - 6\n","                            else:\n","                                start = 0\n","                            if len(aw) > idx + 5:\n","                                end = idx + 6\n","                            else:\n","                                end = len(aw)\n","                            #print(\"************* Found\")\n","                            return \" \".join(aw[start:end])\n","             \n","            # Check if \"what\" appears in the question\n","            if \"what\" in question:\n","                root_token = None\n","                nsubj = []\n","                for token in doc2:\n","                    if token.dep_ == \"ROOT\":\n","                        if token.lemma_ != \"be\":\n","                            root_token = token\n","                    if token.dep_ == \"nsubj\" or token.dep_ == \"nsubjpass\":\n","                        nsubj = nsubj + [t.lemma_ for t in list(token.lefts) if not t.is_stop] + [token.lemma_] + [t.lemma_ for t in list(token.rights) if not t.is_stop]\n","                \n","                # Remove \"the\"\n","                times = 0\n","                for ns in nsubj:\n","                    if ns == \"the\":\n","                        times += 1\n","                for i in range(times):\n","                    nsubj.remove(\"the\")\n","\n","                all_pos_words = []\n","                if root_token is not None:\n","                    all_pos_words.append(root_token.lemma_)\n","                    all_pos_words = all_pos_words + nsubj\n","            \n","                # Check if the root or nsubj or nsubjpass is found\n","                found_it = False\n","                start = -1\n","                end = -1\n","                for token in doc1:\n","                    checkx = False\n","                    for aw in all_pos_words:\n","                        if aw in token.text.lower():\n","                            checkx = True\n","                    if checkx or token.lemma_ in all_pos_words:\n","                        if not found_it:\n","                            start = max(0, token.i - 3)\n","                            found_it = True\n","                        end = token.i + 8\n","                        if len(doc1) <= end:\n","                            end = len(doc1)\n","                            if doc1[-1].text == \".\":\n","                                end = len(doc1) - 1\n","\n","                if found_it:\n","                    #print(\"Got >>>>>>>>>>>>>>>>>>> what?\")\n","                    #print(all_pos_words)\n","                    #print(ans_sentence)\n","                    if end - start + 1 < 10:\n","                        start = max(0, end - 9)\n","                    f_possible_ans = []\n","                    for pans in possible_ans:\n","                        if pans in doc1[start:end].text:\n","                            f_possible_ans.append(pans)\n","                    return \" \".join(f_possible_ans)\n","            try:\n","                #print(\"Just join !!!!!!!!!!!!!!!!!!!!\")\n","                #print(ans_sentence)\n","                #print()\n","                return str(\" \".join(possible_ans))\n","            except:\n","                try:\n","                    return str(\" \".join([chunk.merge().text for chunk in doc1.noun_chunks]))\n","                except:\n","                    return \"Not Found\"\n","    except:\n","        doc1 = nlp(ans_sentence)\n","        doc2 = nlp(question)\n","        possible_ans = [i.merge().text for i in [chunk for chunk in doc1.noun_chunks] if not i in [chunk for chunk in doc2.noun_chunks]]\n","           \n","        # Look for What X and What X of Y\n","        x, y = check_what_x(question)\n","        if x is not None:\n","            for idx, a in enumerate(possible_ans):\n","                if x in a or (y is not None and y in a):\n","                    #print(\"!!!!!!!!!!!!!!!!!!!!\")\n","                    #print(possible_ans)\n","                    # Get 1 chunk before and after\n","                    if idx > 1:\n","                        start = idx - 1\n","                    elif idx == 1:\n","                        start = 1\n","                    else:\n","                        start = 0\n","                    if len(possible_ans) > idx + 2:\n","                        end = idx + 2\n","                    elif len(possible_ans) > idx + 1:\n","                        end = idx + 2\n","                    else:\n","                        end = idx + 1\n","                    return \" \".join(possible_ans[start:end])\n","\n","        # Check for \"stand for\" and \"refer to\"\n","        found_stand_refer = False\n","        key_word = []\n","        for token in doc2:\n","            if token.dep_ == \"ROOT\" and token.lemma_ in [\"refer\", \"stand\"]:\n","                found_stand_refer = True\n","            if token.dep_ == \"nsubj\":\n","                key_word = [t.text for t in list(token.lefts) if not t.is_stop] + [token.text] + [t.text for t in list(token.rights) if not t.is_stop]\n","        if found_stand_refer:\n","            aw = word_tokenizer.tokenize(ans_sentence)\n","            #print(ans_para)\n","            for idx, a in enumerate(aw):\n","                for kw in key_word:\n","                    if kw in a:\n","                        #print(\"!!!!!!!!!!!!!!!!!!!!\")\n","                        if idx > 5:\n","                            start = idx - 6\n","                        else:\n","                            start = 0\n","                        if len(aw) > idx + 5:\n","                            end = idx + 6\n","                        else:\n","                            end = len(aw)\n","                        #print(\"************* Found\")\n","                        return \" \".join(aw[start:end])\n","               \n","        if \"what\" in question:\n","            root_token = None\n","            nsubj = []\n","            for token in doc2:\n","                if token.dep_ == \"ROOT\":\n","                    if token.lemma_ != \"be\":\n","                        root_token = token\n","                if token.dep_ == \"nsubj\" or token.dep_ == \"nsubjpass\":\n","                    nsubj = nsubj + [t.lemma_ for t in list(token.lefts) if not t.is_stop] + [token.lemma_] + [t.lemma_ for t in list(token.rights) if not t.is_stop]\n","            \n","            times = 0\n","            for ns in nsubj:\n","                if ns == \"the\":\n","                    times += 1\n","            for i in range(times):\n","                nsubj.remove(\"the\")\n","            \n","            all_pos_words = []\n","            if root_token is not None:\n","                all_pos_words.append(root_token.lemma_)\n","                all_pos_words = all_pos_words + nsubj\n","            found_it = False\n","            start = -1\n","            end = -1\n","            for token in doc1:\n","                checkx = False\n","                for aw in all_pos_words:\n","                    if aw in token.text.lower():\n","                        checkx = True\n","                if checkx or token.lemma_ in all_pos_words:\n","                    if not found_it:\n","                        start = max(0, token.i - 3)\n","                        found_it = True\n","                    end = token.i + 8\n","                    if len(doc1) <= end:\n","                        end = len(doc1)\n","                        if doc1[-1].text == \".\":\n","                            end = len(doc1) - 1\n","            \n","            if found_it:\n","                #print(\"Got >>>>>>>>>>>>>>>>>>> what?\")\n","                #print(all_pos_words)\n","                #print(ans_sentence)\n","                if end - start + 1 < 10:\n","                        start = max(0, end - 9)\n","                f_possible_ans = []\n","                for pans in possible_ans:\n","                    if pans in doc1[start:end].text:\n","                        f_possible_ans.append(pans)\n","                return \" \".join(f_possible_ans)\n","             \n","        try:\n","            #print(\"Just join !!!!!!!!!!!!!!!!!!!!\")\n","            #print(ans_sentence)\n","            #print()\n","            return str(\" \".join(possible_ans))\n","        except:\n","            try:\n","                return str(\" \".join([chunk.merge().text for chunk in doc1.noun_chunks]))\n","            except:\n","                return \"Not Found\"\n","    \n","    \n","print(\"Done\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"metadata":{"id":"NxEe0WEDMyNL","colab_type":"text"},"cell_type":"markdown","source":["## Run on training data"]},{"metadata":{"id":"igYUoyEZMyNM","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","Description: Run solver on the training data\n","Input:\n","Output:\n","'''\n","def run_training():\n","    global training_answer_data\n","    \n","    total_questions = 0\n","    total_correct = 0\n","    \n","    # Each training question\n","    for q, t in enumerate(training_answer_data[0:2000]):\n","        # (1) classify answer type\n","        ans_type, ans_type_info = classify_answer_type(t[\"question\"])\n","        \n","        if ans_type != \"unkown_type\" or not \"what\" in t[\"question\"]:\n","            # Skip\n","            continue\n","        \n","        print(t[\"question\"])\n","        \n","        print(\"Q\" + str(q) + \": \" + ans_type)\n","        # (2) Find top-k matched paragraphs\n","        topk_paragraphs, topk_scores = get_top_k_paragraphs(3, t[\"docid\"], t[\"question\"])\n","        #print(topk_paragraphs)\n","        # (3) Call main solver\n","        if ans_type == \"unkown_type\":\n","            prediction = predict_others(t[\"question\"], topk_paragraphs)\n","        else:\n","            prediction = main_solve(t[\"question\"], topk_paragraphs, topk_scores, ans_type, ans_type_info)\n","            if prediction == \"Not Found\":\n","                prediction = predict_others(t[\"question\"], topk_paragraphs)\n","            \n","        total_questions += 1\n","        # Count\n","        if check_answer(t[\"text\"], prediction):\n","            total_correct += 1\n","            print(\"Correct\")\n","        else:\n","            print(\"Wrong\")\n","    \n","    print(\"Total Questions = \" + str(total_questions))\n","    print(\"Total Correct = \" + str(total_correct))\n","\n","#run_training()\n","       "],"execution_count":0,"outputs":[]},{"metadata":{"id":"DgslDOI6MyNO","colab_type":"text"},"cell_type":"markdown","source":["## Run on dev data"]},{"metadata":{"id":"rouq5zcmMyNP","colab_type":"code","colab":{}},"cell_type":"code","source":["from tqdm import tqdm\n","\n","def run_devl():\n","    total_questions = 0\n","    total_correct = 0\n","    result = []\n","    global development_answer_data\n","        \n","    # Each training question\n","    for q, t in tqdm(enumerate(development_answer_data)):\n","        # (1) classify answer type\n","        ans_type, ans_type_info = classify_answer_type(t[\"question\"])\n","        # (2) Find top-k matched paragraphs\n","        topk_paragraphs, topk_scores = get_top_k_paragraphs(3, t[\"docid\"], t[\"question\"])\n","        #print(topk_paragraphs)\n","        # (3) Call main solver\n","\n","        prediction = \"\"\n","        if ans_type == \"unkown_type\":\n","            prediction = predict_others(t[\"question\"], topk_paragraphs)\n","            result.append((q,prediction.lower()))\n","        elif ans_type == \"what_hypernym_type\":\n","            prediction = main_solve(t[\"question\"], topk_paragraphs, topk_scores, ans_type, ans_type_info)\n","            if prediction == \"Not Found\":\n","                prediction = predict_others(t[\"question\"], topk_paragraphs)\n","            result.append((q,prediction.lower()))\n","\n","        if ans_type == \"what_hypernym_type\":\n","            total_questions += 1\n","            # Count\n","            if check_answer(t[\"text\"], prediction):\n","                total_correct += 1\n","                print(\"Correct\")\n","            else:\n","                print(\"Wrong\")\n","    \n","    print(\"Total Questions = \" + str(total_questions))\n","    print(\"Total Correct = \" + str(total_correct))\n","    \n","    return result\n","#result_devl = run_devl()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3fjDkPaVMyNS","colab_type":"text"},"cell_type":"markdown","source":["## Evaluation on dev data"]},{"metadata":{"id":"-4m-1DxDMyNT","colab_type":"code","colab":{}},"cell_type":"code","source":["def evaluate_dev():\n","    total_correct = 0\n","    for i in range(len(result_devl)):\n","        if result_devl[i][1] == development_answer_data[i][\"text\"]:\n","            total_correct += 1\n","    print(total_correct)\n","\n","# evaluate()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oiWF2brfMyNW","colab_type":"text"},"cell_type":"markdown","source":["## Run on testing data"]},{"metadata":{"id":"ZfxULgAfMyNX","colab_type":"code","colab":{},"outputId":"2832c41a-4d64-4596-dbe5-40cc34354e62"},"cell_type":"code","source":["from tqdm import tqdm\n","\n","def run_testing():\n","    result = []\n","    global testing_answer_data\n","        \n","    # Each training question\n","    for q, t in tqdm(enumerate(testing_answer_data)):\n","        # (1) classify answer type\n","        ans_type, ans_type_info = classify_answer_type(t[\"question\"])\n","        # (2) Find top-k matched paragraphs\n","        topk_paragraphs, topk_scores = get_top_k_paragraphs(3, t[\"docid\"], t[\"question\"])\n","        #print(topk_paragraphs)\n","        # (3) Call main solver\n","\n","        if ans_type == \"unkown_type\":\n","            prediction = predict_others(t[\"question\"], topk_paragraphs)\n","            result.append((q,prediction.lower()))\n","        else:\n","            prediction = main_solve(t[\"question\"], topk_paragraphs, topk_scores, ans_type, ans_type_info)\n","            if prediction == \"Not Found\":\n","                prediction = predict_others(t[\"question\"], topk_paragraphs)\n","            result.append((q,prediction.lower()))\n","\n","    return result\n","result = run_testing()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["85it [00:09,  8.88it/s]C:\\Users\\winnc\\Anaconda3\\lib\\site-packages\\tqdm\\_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n","  TqdmSynchronisationWarning)\n","3618it [07:24,  8.13it/s]\n"],"name":"stderr"}]},{"metadata":{"id":"KHHfw--zMyNa","colab_type":"text"},"cell_type":"markdown","source":["## Write data to csv file for submission"]},{"metadata":{"id":"AZWj6zXwMyNb","colab_type":"code","colab":{},"outputId":"cc0c0629-5711-44ee-cd76-23e0421b51d3"},"cell_type":"code","source":["save_prediction_to_csv(result, \"winn_xudong\")\n","print(\"Done\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"metadata":{"id":"b_Z6D2VwMyNe","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}